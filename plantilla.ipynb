{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.read_csv(\"./data/Regresión_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.isnull().sum() / datos.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.isna().sum() / datos.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos[\"dolor_NRS\"].value_counts()/datos.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos[\"Duracion_KTAS_Min\"] = datos[\"Duracion_KTAS_Min\"].str.replace(',', '.').astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heat map\n",
    "plt.figure(figsize=(12, 10))\n",
    "cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "\n",
    "sns.heatmap(\n",
    "    datos[numericas].corr(),\n",
    "    cmap=cmap,\n",
    "    vmin=-1, vmax=1,\n",
    "    annot=True\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos[\"Sexo_stan\"] = datos[\"Sexo\"].apply(lambda x: 0 if x == 2 else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se eliminan los registros totalmente duplicados\n",
    "datos = datos.dropna(subset=[\"Duracion_Estancia_Min\"]+candidatas)\n",
    "datos[[\"Duracion_Estancia_Min\"]+candidatas].isnull().sum() / datos.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.loc[datos.duplicated(subset=candidatas, keep=False)][[\"Duracion_Estancia_Min\"]+candidatas].head(10)\n",
    "duplicated_rows = datos.loc[datos.duplicated(subset=candidatas, keep=False)].shape[0]\n",
    "duplicated_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duplicates: {(duplicated_rows/total_rows)*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.loc[datos.duplicated(subset=candidatas+[\"Duracion_Estancia_Min\"], keep=False)].tail(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_rows = datos.loc[datos.duplicated(subset=candidatas+[\"Duracion_Estancia_Min\"], keep=False)].shape[0]\n",
    "duplicated_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.drop_duplicates(subset=candidatas, inplace=True)\n",
    "datos.drop_duplicates(subset=candidatas+[\"Duracion_Estancia_Min\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGRESION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(datos_recorte[candidatas], datos_recorte[\"Duracion_Estancia_Min\"], test_size=0.3, random_state=1)\n",
    "X_train.shape, y_train.shape\n",
    "X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = LinearRegression()\n",
    "regression.fit(X_train, y_train)\n",
    "pd.DataFrame({\"columns\": candidatas, \"coef\": regression.coef_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1, len(candidatas), sharey=True, figsize=(20, 5), layout=\"constrained\")\n",
    "\n",
    "for i in range(len(candidatas)):\n",
    "    col = candidatas[i]\n",
    "    x = X_train[col]\n",
    "    m = regression.coef_[i]\n",
    "    b = regression.intercept_\n",
    "\n",
    "    axs[i].plot(x, y_train, \"o\", alpha=0.1)\n",
    "    axs[i].plot(x, x * m + b)\n",
    "    axs[i].set_title(col)\n",
    "print(\"Train:\", np.sqrt(mean_squared_error(y_train, regression.predict(X_train))))\n",
    "print(\"Test:\", np.sqrt(mean_squared_error(y_test, regression.predict(X_test))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(20,10))\n",
    "ax = sns.boxplot(data=datos_cols_selec, orient=\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler()\n",
    "\n",
    "\n",
    "datos_prep_norm = datos_cols_selec.copy()\n",
    "datos_prep_norm[name_cols] = mms.fit_transform(datos_cols_selec[name_cols])\n",
    "datos_prep_norm=datos_prep_norm[name_cols]\n",
    "saved_cols = datos_prep_norm.columns\n",
    "\n",
    "datos_prep_norm = pd.DataFrame(datos_prep_norm, columns =saved_cols)\n",
    "print(datos_prep_norm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distortion(data,\n",
    "                    k_min=1, \n",
    "                    k_max=11,\n",
    "                    ylabel = 'Distortion',\n",
    "                    xlabel = 'Number of clusters',\n",
    "                    title = 'Distortion Plot'):\n",
    "    distortions = []\n",
    "    for i in range(k_min, k_max):\n",
    "        km = KMeans(n_clusters=i,\n",
    "                 init='k-means++',\n",
    "                 n_init=10,\n",
    "                 max_iter=300,\n",
    "                 random_state=0)\n",
    "        km.fit(data)\n",
    "        distortions.append(km.inertia_)\n",
    "    plt.plot(range(k_min,k_max), distortions, marker='o')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_clusters=2\n",
    "kmeans = KMeans(n_clusters=N_clusters, random_state=0) \n",
    "kmeans = kmeans.fit(datos_prep_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "datos_prep_norm['Cluster'] = labels\n",
    "\n",
    "cluster_distrib = datos_prep_norm['Cluster'].value_counts()\n",
    "\n",
    "fig=plt.figure(figsize=(12,8))\n",
    "sns.barplot(x=cluster_distrib.index, y=cluster_distrib.values, color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=datos_prep_norm, hue=\"Cluster\", palette=\"Dark2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_silhouette(data, \n",
    "                    labels,\n",
    "                   ylabel = 'Grupos',\n",
    "                   xlabel = \"Coeficiente de silueta\",\n",
    "                   title = 'Gráfica de silueta'):\n",
    "    cluster_labels = np.unique(labels)\n",
    "    print(cluster_labels)\n",
    "    n_clusters = cluster_labels.shape[0]\n",
    "    silhouette_vals = silhouette_samples(data,\n",
    "                                        labels,\n",
    "                                        metric='euclidean')\n",
    "    y_ax_lower, y_ax_upper = 0, 0\n",
    "    yticks = []\n",
    "    for i, c in enumerate(cluster_labels):\n",
    "        c_silhouette_vals = silhouette_vals[labels == c]\n",
    "        c_silhouette_vals.sort()\n",
    "        y_ax_upper += len(c_silhouette_vals)\n",
    "        color = cm.jet(float(i) / n_clusters)\n",
    "        plt.barh(range(y_ax_lower, y_ax_upper),\n",
    "                        c_silhouette_vals,\n",
    "                        height=1.0,\n",
    "                        edgecolor='none',\n",
    "                        color=color)\n",
    "        yticks.append((y_ax_lower + y_ax_upper) / 2.)\n",
    "        y_ax_lower += len(c_silhouette_vals)\n",
    "    silhouette_avg = np.mean(silhouette_vals)\n",
    "    plt.axvline(silhouette_avg,\n",
    "                color=\"red\",\n",
    "                linestyle=\"--\")\n",
    "    plt.yticks(yticks, cluster_labels+1)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arboles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Ahora definimos la función que nos va a permitir construir nuestra clase.\n",
    "def label_popularity (row):\n",
    "    if row['popularity'] > 33 :\n",
    "        return 1\n",
    "    return 0\n",
    "df_tracks_t['popularity_label']=df_tracks_t.apply (lambda row: label_popularity(row), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se selecciona la variable objetivo, en este caso \"Renuncia\".\n",
    "Y=data_t['Renuncia']\n",
    "# Del conjunto de datos se elimina la variable \"Renuncia\".\n",
    "X=data_t.drop(['Renuncia'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del objeto de arbol de decisión. Utilicemos como criterio de pureza la entropía.\n",
    "arbol = DecisionTreeClassifier(criterion='entropy', max_depth = 4, random_state = 0)\n",
    "arbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo de arbol de decisión con los datos de entrenamiento.\n",
    "arbol = arbol.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinamos las predicciones del modelo sobre el conjunto test.\n",
    "y_pred = arbol.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se genera la matriz de confusión\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "# Se puede visualizar la matriz de confusión\n",
    "#plot_confusion_matrix(arbol, X_test, Y_test)  \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=arbol.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar reporte de clasificación\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importancia= arbol.feature_importances_\n",
    "importancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importancia_atributo = pd.DataFrame(data={\"Atributo\": X_train.columns,\"Importancia\": importancia})\n",
    "importancia_atributo = importancia_atributo.sort_values(by='Importancia', ascending=False).reset_index(drop=True)\n",
    "importancia_atributo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,10))\n",
    "_ = tree.plot_tree(arbol, max_depth=arbol.get_depth(), feature_names=X.columns, class_names=[\"0\", \"1\"], filled=True, fontsize=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hiper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijemos el número de particiones. Utilizaremos K = 10.\n",
    "particiones = KFold(n_splits=10, shuffle=True, random_state = 0)\n",
    "# Establecemos el espacio de búsqueda para los hiperparámetros que deseamos ajustar. \n",
    "param_grid = {'criterion':['gini', 'entropy'],'max_depth':[4,6,8,10,20]}\n",
    "# Definimos el modelo sin ningún valor de estos hiperparámetros\n",
    "arbol = DecisionTreeClassifier(random_state=0)\n",
    "# Ahora utilizamos GridSearch sobre el grid definido y con 10 particiones en la validación cruzada.\n",
    "mejor_modelo = GridSearchCV(arbol, param_grid, cv=particiones)\n",
    "# Ajuste del modelo\n",
    "mejor_modelo.fit(X_train, Y_train)\n",
    "# Podemos ver cuál fue el resultado de la búsqueda (mejores valores de hiperparámetros)\n",
    "mejor_modelo.best_params_\n",
    "print(classification_report(Y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_data, test_size=0.3, random_state=42)\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred = svm_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, normalize=\"true\", values_format=\".0%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
